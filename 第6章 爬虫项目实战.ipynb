{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1网易新闻中心爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#爬取网页的html源码\n",
    "import urllib.request  \n",
    "url = 'http://news.163.com'  \n",
    "head = {}  \n",
    "head['User-Agent'] = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36' \n",
    "req = urllib.request.Request(url, headers=head)\n",
    "response = urllib.request.urlopen(req)\n",
    "h1 = response.read()\n",
    "type(h1)       #显示变量h1的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将python2的默认编码格式设置为utf-8\n",
    "import sys  \n",
    "sys.getdefaultencoding()   #返回程序文件的默认编码格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc4\\xe3\\xc3\\xc7\\xba\\xc3 you are welcome'\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc4 in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bd789f7dc2bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m                      \u001b[1;31m#返回a的类型为bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m                     \u001b[1;31m#输出字节对象a\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m            \u001b[1;31m#将字节对象以utf-8格式解码为字符串，程序报错\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mh2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gbk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc4 in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "#二进制类型与字符串类型的转换\n",
    "s='你们好 you are welcome'   #python3中字符串的默认编码为utf-8\n",
    "type(s)                      #变量s的类型为str\n",
    "a=s.encode('gb2312')         #将字符串按gb2312的编码格式编码为字节对象a \n",
    "type(a)                      #返回a的类型为bytes\n",
    "print(a)                     #输出字节对象a\n",
    "a.decode('utf-8')            #将字节对象以utf-8格式解码为字符串，程序报错\n",
    "\n",
    "h2=h1.decode('gbk')\n",
    "print(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#提取URL与新闻标题\n",
    "import re\n",
    "content=re.findall('a href=\"(http://.+?.html)\">(.+?)</a>',h2)\n",
    "print(content)     #输出列表内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 通过session模拟登陆豆瓣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第一步 定义获取验证码图片，并输入验证码信息的函数get_idcode()\n",
    "import re,requests,time\n",
    "def get_idcode(session,login_url,head,data):\n",
    "    try:\n",
    "        htmls = session.get(login_url,headers=head).text\n",
    "        # 获取验证码图片地址和验证码id\n",
    "        captcha_img=re.findall(img_pattern, htmls)[0]\n",
    "        captcha_id = re.findall(id_pattern, htmls)[0]\n",
    "        # 将验证码图片保存到本地文件\n",
    "        with open('idcode.jpg', 'wb') as f:\n",
    "            captcha_img = session.get(captcha_img) #根据验证码图片地址获取验证码图片\n",
    "            f.write(captcha_img.content)\n",
    "        # 等待用户输入验证码\n",
    "        captcha = input('输入验证码')\n",
    "        # 将验证码和id放在登陆参数上面\n",
    "        data['captcha-solution'] = captcha\n",
    "        data['captcha-id'] = captcha_id\n",
    "    except Exception as e:\n",
    "        print(e,'不需要输入验证码')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第二步 定义登陆函数login()\n",
    "def login(session,login_url,head,post_data):\n",
    "    # 登陆网站，用同一个会话信息，这个post_data就是登陆的参数\n",
    "    session.post(login_url,headers=head,data=post_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第三步 编写爬取影评数据的功能函数\n",
    "def get_html(session,target):\n",
    "    response=session.get(target, headers=head)\n",
    "    response.encoding=\"utf-8\"\n",
    "    return response.text\n",
    "\n",
    "def get_data(html):\n",
    "    comment=re.findall(comment_pattern,html)  #取评论\n",
    "    #score=re.findall('<span class=\"allstar(.+?) rating\"',html)  #取评分\n",
    "    #user=re.findall('<a href=\"https://www.douban.com/people/(.+?)/\"',html) \n",
    "    next_page=re.findall(next_pattern,html) \n",
    "    return comment,next_page\n",
    "\n",
    "def sort_data(info,f):\n",
    "    #user=info[0]\n",
    "    #score=info1]\n",
    "    comment=info[0]\n",
    "    for n in range(len(comment)):\n",
    "        f.write(comment[n].strip()+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第四步 编写入口程序\n",
    "f = open('d:/douban.txt','a',encoding='utf-8')\n",
    "base_url = 'https://movie.douban.com/subject/26752852/comments' \n",
    "first_url='https://movie.douban.com/subject/26752852/comments?status=P'\n",
    "img_pattern = '<img id=\"captcha_image\" src=\"([\\d\\D]*?)\" alt=\"captcha\" class=\"captcha_image\"/>'\n",
    "id_pattern = '<input type=\"hidden\" name=\"captcha-id\" value=\"([\\d\\D]*?)\"/>'\n",
    "comment_pattern='<p class=\"\">([\\d\\D]*?)</p>'\n",
    "next_pattern='<a href=\"(.+?)\" data-page=\"\" class=\"next\">后页 ></a>'\n",
    "head = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36'}\n",
    "login_url='https://accounts.douban.com/login'\n",
    "data = {'source': 'None',\n",
    "    'redir': 'https://www.douban.com/',\n",
    "    'form_email': 'lizhouping_2006@126.com',\n",
    "    'form_password': 'lzp54775477',\n",
    "    'login': '登录'}\n",
    "session = requests.Session()\n",
    "get_idcode(session,login_url,head,data)\n",
    "login(session,login_url,head,data)\n",
    "target=first_url\n",
    "i=0\n",
    "while True:\n",
    "    html=get_html(session,target)\n",
    "    info=get_data(html)\n",
    "    sort_data(info,f)\n",
    "    print(str(i)+' 页下载完毕')\n",
    "    if info[1]:\n",
    "        i=i+1\n",
    "        target = base_url + info[1][0]\n",
    "        html=get_html(session,target)\n",
    "        info=get_data(html)\n",
    "        sort_data(info,f)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print('所有影评下载完毕')\n",
    "        break\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 链家二手房信息爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第一步 编写提取二手房详情页网址的函数\n",
    "import urllib.request,re \n",
    "from bs4 import BeautifulSoup\n",
    "def abs_apart_link(html,key_list):\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "#在类名为title的<div>标签的直接子节点中提取<a>标签\n",
    "    apart_list=soup.select('div.title > a') \n",
    "    for apart in apart_list:\n",
    "        key_list.append(apart.attrs['href'])  #提取Tag对象的href属性值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第二步 编写提取二手房详情页面信息的函数\n",
    "def abs_apart_info(url):\n",
    "    response = urllib.request.urlopen(url,timeout=3)\n",
    "    html =response.read() \n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    #利用正则表达式提取房屋编号\n",
    "    num=re.findall('ershoufang/(.+?).html',url)\n",
    "    f1.write(num[0]+',')\n",
    "    #提取总价标签\n",
    "    total=soup.find_all('span',class_='total')[0].text.strip()\n",
    "    f1.write(total+',')\n",
    "    #提取小区标签\n",
    "    f1.write(soup.select('div.communityName a.info')[0].text.strip()+',') \n",
    "    #提取区域标签\n",
    "    a=soup.select('div.areaName span.info')[0].text.strip()\n",
    "    f1.write(a.replace('\\xa0',' ')+',')\n",
    "    #提取基本属性和交易属性标签\n",
    "    for apart in soup.select('li span.label'):      \n",
    "        f1.write(apart.next_sibling.strip()+',')\n",
    "    f1.write('\\n')\n",
    "    f1.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第三步 遍历并提取100个二手房列表页面中所有二手房源详情页地址\n",
    "key_list=[]\n",
    "url = 'http://sh.lianjia.com/ershoufang/pg'\n",
    "for i in range(100):\n",
    "    print('第'+str(i+1)+'页')\n",
    "    pageurl=url+str(i+1)\n",
    "    try:\n",
    "        response = urllib.request.urlopen(pageurl,timeout=5)\n",
    "        html =response.read()\n",
    "    except:\n",
    "        print(\"爬取失败_____\")\n",
    "        continue\n",
    "    print(\"爬取成功_____\")\n",
    "    abs_apart_key(html,key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第四步提取详情页信息，并写入文本文件中\n",
    "Import re\n",
    "#写入字段名\n",
    "f1 = open('d:/lianjia.txt','a')\n",
    "url0=key_list[0]\n",
    "response = urllib.request.urlopen(url0)\n",
    "html =response.read() \n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "f1.write('房屋编号'+','+'总价'+','+'小区'+','+'区域')\n",
    "#提取基本属性和交易属性的字段名，并写入文件\n",
    "for apart in soup.select('li span.label'):\n",
    "    f1.write(','+apart.text.strip())\n",
    "f1.write('\\n')\n",
    "#写入详情页信息\n",
    "for i in range(len(key_list)):\n",
    "    try:\n",
    "        abs_apart_info(key_list[i])\n",
    "        print(str(i)+' 提取成功')\n",
    "    except:\n",
    "        print(str(i)+' 提取失败')\n",
    "        continue\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 爬取拉勾网json格式数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第一步 定义getPageNumber函数，该函数负责返回搜索页面的总页数\n",
    "import requests,re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import xlwt as ExcelWrite\n",
    "def getPageNumber(url,head,keyword):  \n",
    "    data = {\n",
    "    'first':True,\n",
    "    'pn':1,\n",
    "    'kd':keyword}\n",
    "    page = requests.post(url=url,headers=head,data=data)\n",
    "    page.encoding = 'utf-8'\n",
    "    page_json = page.json()\n",
    "    pageNumbe = int(page_json['content']['positionResult']['totalCount']/15)\n",
    "    return pageNumbe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第二步 定义get_page函数，返回指定搜索页的职位基本信息\n",
    "def get_page(pn,url,head,keyword):\n",
    "    if pn == 1:\n",
    "        first = True\n",
    "    else:\n",
    "        first = False\n",
    "    data = {\n",
    "    'first':first,\n",
    "    'pn':pn,\n",
    "    'kd':keyword}\n",
    "    page = requests.post(url=url,headers=head,data=data,timeout=5)\n",
    "    page.encoding = 'utf-8'\n",
    "    page_json = page.json()\n",
    "    position = page_json['content']['positionResult']['result']\n",
    "    return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第三步 定义get_description函数，返回职位的详细描述信息\n",
    "def get_description(jobID,head):\n",
    "    job_url='https://www.lagou.com/jobs/'+str(jobID)+'.html'\n",
    "    page=requests.get(job_url,headers=head,timeout=5)\n",
    "    page.encoding = 'utf-8'\n",
    "    soup=BeautifulSoup(page.text,'html.parser')\n",
    "    job_description = soup.select('dd[class=\"job_bt\"]')\n",
    "    job_description = str(job_description[0])\n",
    "    rule = re.compile(r'<[^>]+>')\n",
    "    result = rule.sub('', job_description)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第四步 定义save_excel函数，将每条职位信息保存到excel文件中\n",
    "def save_excel(i0,job,sheet,tags,head):\n",
    "    for j in range(len(tags)-1):         \n",
    "        sheet.write(i0,j,str(job[tags[j]]))\n",
    "    try:\n",
    "        des=get_description(job['positionId'],head)\n",
    "        print(str(job['positionId'])+ '职位描述抓取成功')        \n",
    "    except:\n",
    "        print(str(job['positionId'])+ '职位描述抓取失败')\n",
    "        des=''\n",
    "    sheet.write(i0,len(tags)-1,des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第五步 编写入口程序\n",
    "url='https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false&isSchoolJob=0'  #搜索页面的URL地址\n",
    "key='数据'                     #定义搜索的关键字\n",
    "head1={省略......}\n",
    "head2={省略......}\n",
    "pagenumber=getPageNumber(url,head1,key)\n",
    "tags=['positionId','positionName','city', 'createTime','companyFullName','education',\n",
    "'industryField','companySize','firstType','secondType','salary',\n",
    "'workYear','description']\n",
    "book = ExcelWrite.Workbook(encoding='utf-8')     #存放职位情况的excel表格\n",
    "sheet = book.add_sheet('sheet')\n",
    "i0=0\n",
    "for j in range(len(tags)):     #向工作表写入表头\n",
    "    sheet.write(i0,j,str(tags[j]))\n",
    "for i in range(pagenumber):    #将每个职位信息写入工作表\n",
    "    try:\n",
    "        jobs=get_page(i+1,url,head1,key)\n",
    "        print('第'+str(i+1)+'爬取成功')\n",
    "        for job in jobs:\n",
    "            i0=i0+1\n",
    "            save_excel(i0,job,sheet,tags,head2)\n",
    "    except:\n",
    "        print('第'+str(i+1)+'爬取失败') \n",
    "book.save(\"f:\\\\lagou.xls\" )    #保存excel文件"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
